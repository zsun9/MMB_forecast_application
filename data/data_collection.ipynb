{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    #'GDPC1', 'GDPCTPI', 'PCEC', 'FPI', 'PRFI', 'PNFI', 'PCND', 'PCESV', 'PCDG', 'PCNDGC96', 'PCESVC96', 'PCDGCC96', 'COMPNFB', 'PRS85006023', 'BOGZ1FL144104005Q', 'HMLBSHNO', # quarterly\n",
    "    # 'CE16OV', 'CNP16OV', 'AWHNONAG', 'UNRATE', 'CPIAUCSL', 'PCE', 'PCENDC96', 'PCEDG', 'PCEDGC96', 'PCES', 'PCESC96', # monthly\n",
    "    # 'DFF', 'DBAA', 'DGS10', # daily\n",
    "    'PNFIC1', 'JTSJOL',\n",
    "]\n",
    "api = '373b8581900f3b2c94da355762d31d7f',\n",
    "start_date = '1960-01-01',\n",
    "end_date = '2020-03-31',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, warnings, pathlib, time\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.simplefilter('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'api_key': api,\n",
    "    'file_type': 'json',\n",
    "    'observation_start': start_date, # date of interest\n",
    "    'realtime_start': start_date, # start of a period (publication date)\n",
    "    'realtime_end': end_date, # end of a period (one day before the next publication date)\n",
    "    }\n",
    "\n",
    "description_keys = [\n",
    "    'id', \n",
    "    'title', \n",
    "    'frequency', \n",
    "    'frequency_short', \n",
    "    'units',\n",
    "    'units_short',\n",
    "    'seasonal_adjustment',\n",
    "    'seasonal_adjustment_short',\n",
    "    'notes',\n",
    "    'observation_start',\n",
    "    'observation_end',\n",
    "]\n",
    "\n",
    "timeout = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url, params):\n",
    "    '''Download page from ALFRED and check whether the download succeeds'''\n",
    "    page = requests.get(url, params=params, timeout=timeout)\n",
    "    if page.status_code == 429:\n",
    "        time.sleep(10)\n",
    "        page = requests.get(url, params=params, timeout=timeout)\n",
    "    assert page.status_code == 200, f\"No {params['series_id']} from {url}, {page.status_code} error\"\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_or_nan(x):\n",
    "    '''Convert a string to either a float number or NaN'''\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "\n",
    "for variable in variables:\n",
    "    \n",
    "    params.update({'series_id': variable})\n",
    "    description = dict()\n",
    "\n",
    "    # retrieve the basic description (if exists; if multiple descriptions, retreive the last only)\n",
    "    page = download_page(url='https://api.stlouisfed.org/fred/series', params=params)\n",
    "    page = page.json()['seriess'][-1]\n",
    "    description = {key: page.get(key, '') for key in description_keys}\n",
    "\n",
    "    # retreive the release (if exists)\n",
    "    page = download_page(url='https://api.stlouisfed.org/fred/series/release', params=params)\n",
    "    page = page.json()['releases'][-1]\n",
    "    release_id = page['id']\n",
    "    description['release'] = page.get('name', '')\n",
    "    description['release_url'] = page.get('link', '')\n",
    "    \n",
    "    # retreive the source (if exists)\n",
    "    params_source = params\n",
    "    page = requests.get(\n",
    "        url='https://api.stlouisfed.org/fred/release/sources', \n",
    "        params={'api_key': api, 'file_type': 'json', 'release_id': release_id,}\n",
    "        )\n",
    "    page = page.json()['sources'][-1]\n",
    "    description['source'] = page.get('name', '')\n",
    "    description['source_url'] = page.get('link', '')\n",
    "\n",
    "    # for variables updated in daily frequency, download the last vintage only\n",
    "    if description['frequency_short'] == 'D':\n",
    "        params.update({'realtime_start': end_date})\n",
    "    else:\n",
    "        params.update({'realtime_start': start_date})\n",
    "\n",
    "    # retreive the vintage dates (if available)\n",
    "    page = download_page(url='https://api.stlouisfed.org/fred/series/vintagedates', params=params)\n",
    "    vintage_dates = page.json()['vintage_dates']\n",
    "    description['numberof_vintage_dates'] = len(vintage_dates)\n",
    "    description['is_revised'] = len(vintage_dates) > 1\n",
    "    description['vintage_dates'] = vintage_dates\n",
    "    \n",
    "    descriptions.append(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data descriptions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('raw_variables_descriptions.txt', 'w') as json_file:\n",
    "#     json.dump(descriptions, json_file, indent=4)\n",
    "\n",
    "try: \n",
    "\n",
    "    pd_descriptions_old = pd.read_csv('raw_variable_description.csv')\n",
    "    old_variable_set = pd_descriptions_old['id'].to_list()\n",
    "\n",
    "    pd_descriptions_new = pd.DataFrame(descriptions)\n",
    "    new_variable_set = pd_descriptions_new['id'].to_list()\n",
    "\n",
    "    # check whether new variables already exist\n",
    "    duplicated_variables = [variable for variable in new_variable_set if variable in old_variable_set]\n",
    "    if len(duplicated_variables) > 0:\n",
    "        warnings.warn('\\nWARNING: Some new raw variables already exist and their old information will be removed!\\n')\n",
    "\n",
    "    # drop duplicated variables\n",
    "    pd_descriptions_old.drop(\n",
    "        pd_descriptions_old.index[pd_descriptions_old['id'].map(lambda x: x in duplicated_variables)],\n",
    "        inplace = True\n",
    "        )\n",
    "\n",
    "    # concatenate and save to disk\n",
    "    pd_descriptions = pd.concat([pd_descriptions_old, pd_descriptions_new])\n",
    "    pd_descriptions.to_csv('raw_variable_description.csv', index=False)\n",
    "\n",
    "except:\n",
    "\n",
    "    pd.DataFrame(descriptions).to_csv('raw_variable_description.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and re-organize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a32cca48b9194cb881a0851d28892acf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-fdca542a9b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# for variables updated in daily frequency, download the last vintage only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpd_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd_descriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frequency_short'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'D'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'realtime_start'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for variable in variables:\n",
    "\n",
    "    # for variables updated in daily frequency, download the last vintage only\n",
    "    if pd_descriptions[pd_descriptions['id']==variable]['frequency_short'].values[0] == 'D':\n",
    "        params.update({'realtime_start': end_date})\n",
    "    else:\n",
    "        params.update({'realtime_start': start_date})\n",
    "\n",
    "    # download data from ALFRED\n",
    "    params.update({'series_id': variable})\n",
    "    page = download_page(url='https://api.stlouisfed.org/fred/series/observations', params=params)\n",
    "\n",
    "    # convert data type from JSON -> DataFrame\n",
    "    # convert values from string -> float, convert dates from string -> datetime\n",
    "    data = pd.DataFrame(page.json()['observations'])\n",
    "    data['value'] = data['value'].map(lambda x: float_or_nan(x))\n",
    "    for column in data.columns:\n",
    "        if column != 'value':\n",
    "            data[column] = pd.to_datetime(data[column])\n",
    "\n",
    "    # collect all vintage dates\n",
    "    vintage_dates = sorted(list(set(data['realtime_start'].to_list())))\n",
    "\n",
    "    # reshape data structure to ['observation_date', 'VarName_VintDate1', 'VarName_VintDate2', ...]\n",
    "    for index, (observation_date, group) in notebook.tqdm(enumerate(data.groupby('date'))):\n",
    "\n",
    "        if index == 0:\n",
    "\n",
    "            temp_values = {'observation_date': observation_date.strftime('%Y-%m-%d')}\n",
    "\n",
    "            for vintage_date in vintage_dates:\n",
    "                found_value = False\n",
    "                for _, row in group.drop('date', axis=1).iterrows():\n",
    "                    if row['realtime_start'] <= vintage_date <= row['realtime_end']:\n",
    "                        temp_values[f\"{variable}_{vintage_date.strftime('%Y%m%d')}\"] = row['value']\n",
    "                        found_value = True\n",
    "                        break\n",
    "                if found_value == False:\n",
    "                    temp_values[f\"{variable}_{vintage_date.strftime('%Y%m%d')}\"] = float('nan')\n",
    "            assert len(temp_values) == len(vintage_dates) + 1\n",
    "\n",
    "            data_output = pd.DataFrame(temp_values, index=[0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp_values = [observation_date.strftime('%Y-%m-%d')]\n",
    "\n",
    "            for vintage_date in vintage_dates:\n",
    "                found_value = False\n",
    "                for _, row in group.drop('date', axis=1).iterrows():\n",
    "                    if row['realtime_start'] <= vintage_date <= row['realtime_end']:\n",
    "                        temp_values.append(row['value'])\n",
    "                        found_value = True\n",
    "                        break\n",
    "                if found_value == False:\n",
    "                    temp_values.append(float('nan'))\n",
    "            assert len(temp_values) == len(vintage_dates) + 1\n",
    "\n",
    "            data_output.loc[data_output.shape[0]] = temp_values\n",
    "\n",
    "    data_output.set_index('observation_date', inplace=True)\n",
    "    data_output.to_csv(f'raw_variables/{variable}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitba3d8e8fc21c4b4c8d5f629dcc40dfe0",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}